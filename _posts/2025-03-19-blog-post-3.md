---
title: 'perturb-seq 数据分析：细胞系的异质性如何去除'
date: 2025-03-19
permalink: /posts/2025/03/blog-post-3/
tags:
  - perturbseq analysis
  - python
  - cell line, heterogeneity
---

perturbseq的数据在分析的时候，发现所有的细胞（敲除的、没敲除的）在umap上混在一起。本来期待的敲除对细胞造成的表达量的影响，却不是最终观测到的最大的差别，那么这些差别是什么呢？

## 细胞周期的影响
细胞系在培养的时候，细胞处于不同的细胞周期阶段，细胞周期的差异，是细胞背景差异的原因之一。

## 细胞的异质性
因为使用的是细胞系，细胞系中细胞之间的差异可能比较大，尤其是在实验过程中，添加实验条件之后连续培养多天，经过这段时间的增殖之后，组内细胞之间的差异可能会更大。

## 深度学习的方法去除
目前看见的两个方法
* contrastiveVI(使用变分自编码器，学习none-perturbed control cell的内部差异作为背景，在细胞间的比较时去除该差异)
* scCAPE(使用对抗学习和随机森林，学习none-perturbed control cell的内部差异作为背景，在细胞间的比较时去除该差异)

scVI 生成模型的生成过程
------
最近看了contrastiveVI的核心代码。contrastiveVI基于scVI的框架，其中decoder的部分是直接使用的scVI库中的decoder类。我们可以从scVI和contrastiveVI的文章看出来，这两种方法都是基于零膨胀负二项分布的单细胞counts数据生成模型。


<!--
This block of code works as well, uses the below one instead
\\[ z_n \sim {\mathrm{Normal}}( {0,I} ) \\] 
\\[ \ell_n \sim {\mathrm{LogNormal}} ( {\ell_\mu ,\ell_{\sigma}^2}) \\] 
\\[ \rho_n = f_w( {z_n,s_n}) \\]
\\[ w_{ng} \sim {\mathrm{Gamma}}( {\rho_n^g,\theta } ) \\]
\\[ y_{ng} \sim {\mathrm{Poisson}}( {\ell_n w_{ng}} ) \\]
\\[ h_{ng} \sim {\mathrm{Bernoulli}}( {f_h^g( {z_n,s_n} )} ) \\]
<p>$$ x_{ng}  = \{  \displaylines{ y_{ng}\ \ \text{if}\ h_{ng} = 0 \\\  0 \ \ \text{otherwise} } $$</p>
-->


<p>\begin{array}{ll} 
  {z_n} & {\sim {\mathrm{Normal}}\left( {0,I} \right)} \\ 
  {\ell _n} & {\sim {\mathrm{log}}\,{\mathrm{normal}}\left( {\ell _\mu ,\ell _\sigma ^2} \right)} \\
  {\rho _n} & { = f_w\left( {z_n,s_n} \right)} \\ 
  {w_{ng}} & {\sim {\mathrm{Gamma}}\left( {\rho _n^g,\theta } \right)} \\ 
  {y_{ng}} & {\sim {\mathrm{Poisson}}\left( {\ell _nw_{ng}} \right)} \\ 
  {h_{ng}} & {\sim {\mathrm{Bernoulli}}\left( {f_h^g\left( {z_n,s_n} \right)} \right)} \\ 
  {x_{ng}} & { = \left\{ 
              {\begin{array}{l} 
                {y_{ng}\,\text{if}\,h_{ng} = 0} \\ 
                {0\,\text{otherwise}}\,
              \end{array}} 
            \right.} 
  \end{array}</p>

contrastiveVI在scVI模型的基础上改动的地方：
------
在Variational Auto-Encoder（VAE）模型的基础上（深入了解可以查看文末的链接），contrastiveVI的特点是，将scVI模型中的\\(z_n\\)，替换为两组潜在变量：代表背景差异的\\(c_n\\)和代表实验因素差异的\\(t_n\\)。在训练时对于对照组细胞（non-perturbed control cell），\\(t_n\\)恒定设置为0，其他情况\\(c_n\\)和\\(t_n\\)从标准分布中采样。
对于实验设计矩阵\\(m_{np}\\),


<p>\begin{array}{ll}
{ m_{np} } & { = \left\{ 
              {\begin{array}{l} 
                 {1,\  if\ the \ \textit{n}-th \ cell \ is \ in \ perturbation \ group \  \textit{p} }  \\ 
                 {0, \ otherwise.}\,
              \end{array}} 
              \right. } \\
{ t_n }   & { \sim {\mathrm{Normal}}( {0,I} )  } \\ 
{ t_n }   & { = 0, \ if \ \sum_{p}m_{np}=0 } \\ 
{ c_n }   & { \sim {\mathrm{Normal}}( {0,I} ) } 
  
\end{array}</p>



变分方法寻找证据下确界
------



<p>\begin{array}{ll}
{ p(x, c, t|s) } &  { = p(x|c, t, s) \ p(c,t|s) } \\
{}              &  { = p(x|c, t, s) \ p(c|s) \ p(t|s) } \\
                    \\
{ q(c,t,m|x,s) }   &  { = q(c,m|x,s,t) \ q(t|x,s)   } \\
{}                &  { = q(m|x,s,t) \ q(c|x,s,t) \ q(t|x,s) } \\
{}                &  { = q(m|x,s,t) \ q(c|x,s) \ q(t|x,s) } \\
{}                &  { = q(m|t) \ q(c|x,s) \ q(t|x,s) } \\
                 \\
\end{array}</p>

<p>\begin{array}{ll}
{ \log{ p(x|s)} + \log{q(m|x,s)} }   &  { = \log{\int p(x,c,t|s)dcdt}  + \log{\int q(m,t|x,s) dt} } \\
{}             &   { = \log{ \int q(c,t,m|x,s)  \frac{p(x|c,t,s) \ p(c,t|s) }{q(c,t,m|x,s)} dcdtdm } + \log{ \int   \ q(m|t) \ q(t|x,s) dt }  } \\
  {}             &   { = \log{ \int q(c,t,m|x,s)  \frac{p(x|c,t,s) \ p(c,t|s) }{q(m|t) \ q(c|x,s) \ q(t|x,s)} dcdtdm } + \log{ \int   \ q(m|t) \ q(t|x,s) dt }  } \\
  {}             &   { \geq   \underbrace{  \int q(c,t,m|x,s)  \log{ \frac{ p(x|c,t,s) \ p(c,t|s)}{ q(m|t) \  q(c,t|x,s) }  } dcdtdm +  \int  q(t|x,s) \log{ q(m|t)} dt  }_\textbf{ ELBO (根据Jensen 不等式) }  } \\
{}            &   { = \int q(c,t,m|x,s) \log{  p(x|c,t,s) dcdtdm}  +  \int  q(c,t,m|x,s)  \log{ \frac{ p(c,t|s) }{ q(m|t) \ q(c,t|x,s) } dcdtdm}  +  \int  q(t|x,s) \log{  q(m|t)   dt }   } \\
  {}            &   { = \mathbb{E}_{q(c,t,m|x,s)} \log{ p(x|c,t,s)} - \int q(c,t,m|x,s) \log{ \frac{  q(c,t|x,s) }{ p(c,t|s) } } dcdtdm - \int q(t,m|x,s) \log{  q(m|t)  } dtdm  +  \mathbb{E}_{q(t|x,s)} \log{ q(m|t) } } \\
\\
{}            &   { = \log{ \int q(c,t,m|x,s)  p(x|c,t,s) dcdtdm} + \log{ \int q(c,t|x,s)  \frac{ p(c,t|s)}{q(c,t|x,s) } dcdt} + \log{ \int q(m|t) \ q(m,t|x,s)  dtdm} +    \log{ \int \ q(m|t) \ q(t|x,s)  dt} } \\
{}            &   { = \log{ \int q(c,t,m|x,s)  p(x|c,t,s) dcdtdm} + \log{ \int q(c,t|x,s)  \frac{ p(c,t|s) }{ q(c,t|x,s) } dcdt} + \log{ \int  q(t|x,s)   dt} +    \log{ \int  \ q(m|t) \ q(t|x,s) dt } } \\
{}             &   { \geq   \underbrace{  \int q(c,t,m|x,s)  \log{p(x|c,t,s)} dcdtdm -  \int q(c,t|x,s) \log{ \frac{  q(c,t|x,s) }{ p(c,t|s) } } dcdt +  \int  q(t|x,s) \log{ q(m|t)} dt  }_\textbf{ (ELBO) }  } \\
{}            &   { = \mathbb{E}_{q(c,t,m|x,s)} \log{ p(x|c,t,s)} - \int q(c,t|x,s) \log{ \frac{  q(c,t|x,s) }{ p(c,t|s) } } dcdt +  \mathbb{E}_{q(t|x,s)} \log{ q(m|t) } } \\
{}           &   { = \mathbb{E}_{q(c,t,m|x,s)} \log{ p(x|c,t,s) } - {D}_{KL}(q(c,t|x,s)||p(c,t|s))   +   \mathbb{E}_{q(t|x,s)} \log{ q(m|t) }  } \\
{}           &   { = \mathbb{E}_{q(c,t,m|x,s)} \log{ p(x|c,t,s) }  + \left( { - {D}_{KL}(q(c,t|x,s)||p(c,t)) } \right)   +  \mathbb{E}_{q(t|x,s)} \log{ q(m|t) } } \\
              \\
\end{array}</p>

ELBO的三项分别对应了反符号的三种loss：\\( - Loss_{reconst}，- Loss_{KLDiver} ，- Loss_{Classify} \\)
<p>\begin{array}{ll}
{ - Loss_{reconst} } & {= - MSE(x,\hat{x})} & {对应ELBO第一项} \\ 
{ - Loss_{KLDiver} } & {= - {D}_{KL}(q(t,c|x,s)||p(t,c)) } & {对应ELBO第二项} \\ 
{ - Loss_{Classify}} & {= - CE (m,\hat{m}) } & {对应ELBO第三项} \\ 
\end{array}</p>



最大化ELBO等价于最小化三种loss.其中，由于\\({D}_{KL}(q(t\|x,s)\|\|p(t))\\)对于target cell，\\( t = 0 \\), \\(p(t) \sim Dirac \ distribution\\), 使用Wasserstein distance代替KL divergence。



<p>\begin{array}{ll}
 { {D}_{KL}(q(t,c|x,s)||p(t,c)) } &{  = \left\{ 
                                              {\begin{array}{l} 
                                                {  {D}_{KL}(q(c|x,s)||p(c)) +  {D}_{KL}(q(t|x,s)||p(t)), } & { \text{if cell n is target KO cell} } \\
                                                {  {D}_{KL}(q(c|x,s)||p(c)) +  \textbf{W}_{2}^{2}(q(t|x,s), \delta (t=0) ), } & { \text{if cell n is non-targeting cell} } \\
                                                \end{array}
                                              }
                                        \right.
                            } \\
  {   \textbf{W}_{2}^{2}(q(t|x,s), \delta (t=0) )   } & {  =  \sum{ \sigma_{t}(x,s)^{2} } + \sum{ \mu_{t}(x,s)^{2} }  } \\
\end{array}</p>






scVI的教程中有使用pyro构建新模型的部分，因此，我也尝试了一下使用pyro框架进行contrastiveVI的复现。

## 代码

* 定义深度学习模型

```python

import pyro
import scanpy as sc
from scanpy import AnnData
import scvi
import torch
import torch.nn as nn
import numpy as np
import pyro.distributions as dist 
import pyro.poutine as poutine
from torch.distributions import constraints
import pandas as pd
import pyro
from pyro.infer import SVI, Trace_ELBO
from pyro.optim import Adam
from torch.utils.data import  DataLoader
import scipy
torch.set_default_tensor_type(torch.DoubleTensor)

CUDA_VISIBLE_DEVICES=1
torch.cuda.set_device(1)


if torch.cuda.is_available():
    print("Using GPU")
    torch.set_default_tensor_type("torch.cuda.DoubleTensor")
else:
    print("Using CPU")
    torch.set_default_tensor_type("torch.DoubleTensor")

def make_fc(dims, drop_out):
    layers = []
    for in_dim, out_dim in zip(dims, dims[1:]):
        layers.append(nn.Linear(in_dim, out_dim))
        layers.append(nn.BatchNorm1d(out_dim))
        layers.append(nn.ReLU())
        layers.append(nn.Dropout( drop_out))  
    return nn.Sequential(*layers[:-1])  # Exclude last ReLU


class Encoder(nn.Module):
    def __init__(self, n_input, n_output, n_hidden, drop_out=0.1,transformer=None):
        super(Encoder, self).__init__()
        self.dims = [n_input] + [n_hidden] + [n_output]
        self.fc = make_fc(self.dims, drop_out)
        if transformer is not None:
            self.transformer = transformer
        else:
            self.transformer = None
        self.mean_nn = nn.Linear(self.dims[-1], self.dims[-1])
        self.var_nn  = nn.Linear(self.dims[-1], self.dims[-1])
    def forward(self, x):
        x_ = self.fc(x)
        x_m = self.mean_nn(x_)
        x_v = self.var_nn(x_)
        if self.transformer == 'exp':
            x_v = torch.exp(x_v) +  1e-10
        elif self.transformer == 'sigmoid':
            x_v = torch.sigmoid(x_v)
        elif self.transformer == 'softplus':
            x_v = torch.nn.functional.softplus(x_v) + 1e-10
        elif self.transformer == 'softmax':
            x_v = torch.nn.functional.softmax(x_v, dim=-1)
        elif self.transformer == 'sigmoid,softmax':
            x_v= torch.sigmoid(x_v)
            x_v = torch.nn.functional.softmax(x_v, dim=-1)
        elif self.transformer == 'tanh':
            x_v = torch.tanh(x_v)
        return x_m,x_v


class Classifier(nn.Module):
    def __init__(self, n_input, n_output, n_hidden, drop_out=0,transformer=None):
        super(Classifier, self).__init__()
        self.dims = [n_input] + [n_hidden] + [n_output]
        self.fc = make_fc(self.dims,drop_out)
        if transformer is not None:
            self.transformer = transformer
        else:
            self.transformer = None
    def forward(self, x):
        x_ = self.fc(x)
        if self.transformer == 'softplus':
            x_ = torch.nn.functional.softplus(x_) + 1e-10
        x_ = torch.nn.functional.softmax(x_, dim=-1)
        return x_

from torch.optim import Adam
from pyro.optim import MultiStepLR



class PyroVAE( scvi.module.base.PyroBaseModuleClass):
    def __init__(self, n_latent_t, n_latent_c , n_hidden=100,  use_library_size=True, control_label='Control', batch_size=100, learn_rate=0.001 ):
        super(PyroVAE, self).__init__()
        self.control_label = control_label
        self.use_library_size = use_library_size
        self.n_latent_t = n_latent_t
        self.n_latent_c = n_latent_c
        self.n_hidden = n_hidden
        self.batch_size = batch_size
        self.learn_rate = learn_rate
        self.encoder_z_t = Encoder(self.n_genes , n_latent_t, n_hidden, 0.1,'softplus')  ## encoder for latent variable representing target group effect
        self.encoder_z_c = Encoder(self.n_genes  , n_latent_c, n_hidden,0.1, 'softplus')  ## encoder for latent variable representing intra control cells
        n_latent_z1 = n_latent_t + n_latent_c + self.n_covariates 
        self.decoder_probs = Encoder(n_latent_z1,  self.n_genes , n_hidden, 0, 'softmax') ## decoder for zero gates and probs for each gene
        self.classifier = Classifier( n_latent_t  +  self.n_covariates, self.n_group_onehot , n_hidden) ## classifier for zero gates


    def model(self, data_background, data_target, l_mu, l_sig):
        pyro.module('pyroVAE', self)
        Theta = pyro.param("Theta", 10 * torch.ones(self.n_genes), constraint=constraints.positive )
        ## parse data for control cells 
        (x_c, library_c, covariates_mat_c, group_onehot_c) = ( data_background['x'], data_background['library'], data_background['covariates_mat'] , data_background['group_onehot'])
        with pyro.plate('cell_background', x_c.shape[0]), poutine.scale( scale=1/(x_c.shape[0]  )):
            z_t_c = torch.zeros( torch.Size((x_c.shape[0],self.n_latent_t)))  ## z_t for control cells
        ## parse data for target cells
        (x_t, library_t, covariates_mat_t, group_onehot_t) = ( data_target['x'], data_target['library'], data_target['covariates_mat'] , data_target['group_onehot'])
        with pyro.plate('cell_target', x_t.shape[0]), poutine.scale( scale=1/(x_t.shape[0]  )):
            z_t_loc = torch.zeros( torch.Size((x_t.shape[0],self.n_latent_t)))
            z_t_var = torch.ones( torch.Size((x_t.shape[0],self.n_latent_t)))
            z_t_t = pyro.sample('z_t_t', pyro.distributions.Normal(z_t_loc, z_t_var).to_event(1)) ## sample z_t for target cells
        ## merge data for control and target cells
        x = torch.cat( (x_t, x_c), 0 )
        library = torch.cat( (library_t, library_c), 0 )
        covariates_mat = torch.cat( (covariates_mat_t, covariates_mat_c), 0 )
        group_onehot = torch.cat( (group_onehot_t, group_onehot_c), 0 )
        z_t = torch.cat( (z_t_t, z_t_c), 0 )    # merge z_t for control and target cells
        with pyro.plate('cell_all', x.shape[0]), poutine.scale( scale=1/(x.shape[0]  )):
            z_t_loc = torch.zeros( torch.Size((x.shape[0],self.n_latent_t)))
            z_t_var = torch.ones( torch.Size((x.shape[0],self.n_latent_t)))
            z_c_loc = torch.zeros(torch.Size((x.shape[0],self.n_latent_c)))
            z_c_var = torch.ones(torch.Size((x.shape[0],self.n_latent_c)))
            z_c = pyro.sample('z_c', pyro.distributions.Normal(z_c_loc, z_c_var).to_event(1)) # sample z_c for all cells
            ## sample z_l if not using original library size
            if not self.use_library_size:
                z_l_loc = library.new_zeros(library.size()) + l_mu
                z_l_var = library.new_ones(library.size()) * l_sig
                z_l = pyro.sample('z_l', pyro.distributions.LogNormal(z_l_loc, z_l_var).to_event(1))
            else:
                z_l = library
            ## register reconstruction loss for all cells, scaled with gene number
        #with pyro.plate('cellXgene', x.shape[0]), poutine.scale( scale=1/(x.shape[0] *self.n_genes) ):  ## whether scale reconstruction loss with gene numbers? either way is viable.
            prob_logits_g,prob_p = self.decoder_probs(torch.cat( (z_t,z_c,covariates_mat),1 ))
            prob_logits = ((prob_p * z_l + 1e-10)/(Theta+1e-10) ).log()
            pyro.sample('x', dist.ZeroInflatedNegativeBinomial( total_count=Theta , logits=prob_logits, gate_logits=prob_logits_g).to_event(1), obs=x ) ## register reconstruction loss for all cells

    def guide(self, data_background, data_target, l_mu, l_sig):
        pyro.module('pyroVAE', self)
        ## parse data for control cells
        (x_c,library_c,covariates_mat_c,group_onehot_c) = ( data_background['x'], data_background['library'],data_background['covariates_mat'] , data_background['group_onehot'])
        with pyro.plate('cell_background', x_c.shape[0]), poutine.scale(scale=1/(x_c.shape[0]   )):
            sumx = torch.sum(x_c, dim=-1, keepdim=True)
            x_ = torch.log(x_c/sumx +1)
            z_t_m_c,z_t_v_c = self.encoder_z_t(x_)
            z_t_loss = torch.sum(z_t_m_c **2, dim=-1)  + torch.sum(z_t_v_c **2,dim=-1)
            pyro.factor("z_t_loss", z_t_loss, has_rsample=True) ## register z_t loss for control cells, assumbed to be 0.
        ## parse data for target cells
        (x_t,library_t,covariates_mat_t,group_onehot_t) = ( data_target['x'], data_target['library'],data_target['covariates_mat'] , data_target['group_onehot'])
        with pyro.plate('cell_target', x_t.shape[0]), poutine.scale(scale=1/(x_t.shape[0]  )):
            sumx = torch.sum(x_t, dim=-1, keepdim=True)
            x_ = torch.log(x_t/sumx +1)
            z_t_m_t,z_t_v_t = self.encoder_z_t(x_)
            z_t_t = pyro.sample('z_t_t', pyro.distributions.Normal(z_t_m_t, z_t_v_t).to_event(1)) ## sample z_t for target cells
        ## merge data for control and target cells
        x = torch.cat( (x_t, x_c), 0 )
        library = torch.cat( (library_t, library_c), 0 )
        covariates_mat = torch.cat( (covariates_mat_t, covariates_mat_c), 0 )
        group_onehot = torch.cat( (group_onehot_t, group_onehot_c), 0 )
        z_t_m = torch.cat( (z_t_m_t, z_t_m_c), 0 )  ## merge z_t for control and target cells
        with pyro.plate('cell_all', x.shape[0]), poutine.scale(scale=1/(x.shape[0]  )):
            sumx = torch.sum(x, dim=-1, keepdim=True)
            x_ = torch.log(x/sumx +1 )
            z_c_m,z_c_v = self.encoder_z_c(x_)
            z_c = pyro.sample('z_c', pyro.distributions.Normal(z_c_m, z_c_v).to_event(1)) ## sample z_c for all cells
            ## register classification loss. classification based on mu for z_t, and covariates matrix
            if not self.use_library_size:
                z_l = pyro.sample('z_l', pyro.distributions.LogNormal(l_mu,l_sig).to_event(1) )
            group_prob = self.classifier( torch.cat(( z_t_m, covariates_mat),1) )
            group_loss = nn.CrossEntropyLoss()(group_prob,group_onehot)
            pyro.factor("group_c",  group_loss, has_rsample=False)

    def train(self, epoches=300):
        pyro.clear_param_store()
        if self.scheduler is None:
            scheduler = Adam({'lr': self.learn_rate})
            print('using default scheduler, please specify using set_scheduler method')
        svi = SVI(self.model, self.guide, self.scheduler, loss=Trace_ELBO())
        dataloader = DataLoader(self.dataset, batch_size=self.batch_size, shuffle=True,generator=torch.Generator(device='cuda') )
        for epoch_i in range(epoches):
            i = 0
            for train_batch in dataloader:
                ## parse data
                x = torch.index_select(train_batch, 1, torch.arange(0, PyroVAE.n_genes) )
                library = torch.index_select(train_batch, 1, torch.arange(PyroVAE.n_genes, PyroVAE.n_genes + 1) )
                is_background = torch.index_select(train_batch, 1, torch.arange(PyroVAE.n_genes + 1, PyroVAE.n_genes + 2) )
                covariates_mat = torch.index_select(train_batch, 1, torch.arange(PyroVAE.n_genes + 2, PyroVAE.n_genes + 2 + PyroVAE.n_covariates) )
                group_onehot = torch.index_select(train_batch, 1, torch.arange(PyroVAE.n_genes + 2 + PyroVAE.n_covariates , PyroVAE.n_genes + 2 + PyroVAE.n_covariates  + PyroVAE.n_groups ) )
                data = {}
                if torch.sum(is_background, dim=0) < 5:
                    pass    ## skip batches with few background cells
                else:
                    data['background'] =  { 'x':x[list(is_background[:,0].nonzero()),:], 
                                       'library':library[list(is_background[:,0].nonzero()),:], 
                                       'covariates_mat':covariates_mat[list(is_background[:,0].nonzero()),:], 
                                       'group_onehot':group_onehot[list(is_background[:,0].nonzero()),:] }
                    is_not_background=torch.logical_not(is_background)
                    data['target'] =  { 'x':x[list(is_not_background[:,0].nonzero()),:], 
                                       'library':library[list(is_not_background[:,0].nonzero()),:], 
                                       'covariates_mat':covariates_mat[list(is_not_background[:,0].nonzero()),:], 
                                       'group_onehot':group_onehot[list(is_not_background[:,0].nonzero()),:] }
                    loss = svi.step(  data['background'], data['target'], self.l_mu, self.l_sig)
                    #if i % 10 ==0:
                    print('epoch: ', epoch_i, 'step: ', i, 'loss: ', loss)
                    i += 1
        return svi

    @classmethod
    def set_up_anndata(cls, adata, covariates_col:list, background_col:str, background_label:str , layer='X'):
        if isinstance(adata, AnnData):
            adata = adata
        elif isinstance(adata, str):
            adata = sc.read(adata)
        if layer != 'X':
            data_x = adata.layers[layer].copy()
        else:
            data_x = adata.X.copy()
        cls.n_covariates = len(covariates_col)
        cls.covariates_col = covariates_col
        cls.n_genes = adata.shape[1]
        cls.adata = adata
        cls.background_col = background_col
        cls.background_label = background_label
        cls.n_groups = len( set(  adata.obs[ background_col]) )
        cls.is_background = torch.unsqueeze(torch.tensor(adata.obs[background_col].values == background_label),1)
        cls.group_onehot = torch.tensor(pd.get_dummies( adata.obs[ background_col], dtype=int ).to_numpy())
        cls.n_group_onehot = cls.group_onehot.shape[1]
        cls.x = torch.tensor(data_x.todense() if isinstance(data_x, scipy.sparse.spmatrix) else data_x, dtype=torch.float64) 
        cls.library = torch.sum(cls.x, dim=1, keepdim=True)  #.log() ## library size
        cls.l_mu = torch.mean(cls.library.log(), dim=0) ## mean of library size
        cls.l_sig = torch.std(cls.library.log(), dim=0) ## standard deviation of library size
        #filter_indices = torch.logical_and(library >= l_mu - 3* l_sig ,  library <= l_mu + 3* l_sig).nonzero()[:,0]  ## filter out cells with library size outside of 3 standard deviations
        cls.covariates_mat = torch.tensor(cls.adata.obs[ cls.covariates_col ].to_numpy(), dtype=torch.float64)
        cls.dataset = torch.cat([cls.x, cls.library, cls.is_background, cls.covariates_mat, cls.group_onehot ], dim=1)

    @torch.no_grad()
    def get_latents(self):
        x_ = self.x
        sumx = torch.sum(x_, dim=-1, keepdim=True)
        x_ = torch.log(x_/sumx +1 )
        z_t_m,z_t_v = self.encoder_z_t(x_)
        z_c_m,z_c_v = self.encoder_z_c(x_)
        z_t = pyro.sample('z_t', pyro.distributions.Normal(z_t_m, z_t_v))
        z_c = pyro.sample('z_c', pyro.distributions.Normal(z_c_m, z_c_v))
        Background_latent_variable = torch.cat([z_c_m, z_c_v], dim=1)
        Target_latent_variable = torch.cat([z_t_m, z_t_v], dim=1)
        self.adata.obsm["X_pyro_contra_cm"] = z_c_m.data.cpu().numpy()
        self.adata.obsm["X_pyro_contra_tm"] = z_t_m.data.cpu().numpy()
        self.adata.obsm["X_pyro_contra_c"] = z_c.data.cpu().numpy()
        self.adata.obsm["X_pyro_contra_t"] = z_t.data.cpu().numpy()


    @torch.no_grad()
    def generative_without_z_c(cls, adata, covariates_col:list, background_col:str, background_label:str , layer='X'):
        ''' generating cells with no variance shared intra-control and intra-KO group. for further DEG and gene program analysis
        '''
        pass

    def set_scheduler(self, optimizer, learn_rate, gamma, milestones):
        self.scheduler = MultiStepLR({'optimizer': optimizer,
                                      'optim_args': {'lr': learn_rate},
                                      'gamma': gamma, 'milestones': milestones})



```

* 进行数据的准备（covariates, raw counts, highly variable genes）
  
```python

import scanpy as sc
from scanpy import AnnData
import pandas as pd

dir = '/home/yzeng/proj/Perturb_seq/rerun/'
adata=sc.read(dir+'Clean_Counts.h5ad')
adata.layers['counts'] = adata.X.copy()  # preserve counts
#adata = adata[:,adata.X.sum(axis=0) >= 2000]
adata.obs['batch_wider'] = 1
df2 = pd.DataFrame(adata.obs[['batch','batch_wider']]).pivot(columns='batch').fillna('0')
for obs_add in df2['batch_wider'].columns:
    obs_n = 'batch' + obs_add
    adata.obs[obs_n] = df2['batch_wider'][obs_add].astype(float)
adata.var['mt'] = adata.var_names.str.startswith('MT-')  # annotate the group of mitochondrial genes as 'mt'
sc.pp.calculate_qc_metrics(adata, qc_vars=['mt'], percent_top=None, log1p=False, inplace=True)

sc.pp.highly_variable_genes(adata, layer='counts', flavor='seurat_v3', n_top_genes=2000, subset=True)

covariates_col = ['batch0','batch1','batch2', 'batch3','batch4','batch5','batch6','batch7']

```

* 设置模型，进行训练

```python

PyroVAE.set_up_anndata(adata, covariates_col, 'gene.compact', 'control', 'counts' )
pyro_model = PyroVAE( 80, 80, batch_size=2000 )
pyro_model.set_scheduler(optimizer=Adam, learn_rate=0.01, gamma=0.2, milestones=[10,20,50,100,200])
pyro_model.train( 150 )


pyro_model.get_latents()
pyro_model.adata.obs['perturbed'] = [ 'control' if KO == 'control' else 'perturbed'  for KO in  pyro_model.adata.obs['gene.compact'] ]

```

* 查看target latent

```python

sc.pp.neighbors(pyro_model.adata, use_rep="X_pyro_contra_tm",n_pcs=80)
sc.tl.umap(pyro_model.adata)
sc.pl.umap(pyro_model.adata, color=['phase','perturbed','gene.compact'], frameon=False, show=False)


```
![Umap of target latent variable](https://whuyanzeng.github.io/images/contrastiveVI_pyro.target.png)

* 查看control/background latent

```python

sc.pp.neighbors(pyro_model.adata, use_rep="X_pyro_contra_cm",n_pcs=80)
sc.tl.umap(pyro_model.adata)
sc.pl.umap(pyro_model.adata, color=['phase','perturbed','gene.compact'], frameon=False, show=False)


```
![Umap of target latent variable](https://whuyanzeng.github.io/images/contrastiveVI_pyro.control.png)

## 拓展
目前只写了模型的训练过程以及获取latent variable，还有一些功能没有写，包括基于latent variable的DEG的计算、生成模型的应用等等。
写本模型，涉及到了VAE模型的基础知识,相关的链接：
  1. [https://arxiv.org/pdf/1906.02691](https://arxiv.org/pdf/1906.02691),
  2. [https://spaces.ac.cn/archives/5253](https://spaces.ac.cn/archives/5253),
  3. [https://www.nature.com/articles/s41592-023-01955-3](https://www.nature.com/articles/s41592-023-01955-3),
  4. [scVI](https://www.nature.com/articles/s41592-018-0229-2)和[pyro](https://pyro.ai/examples/)框架。

scVI, pytorch lightning, pyro等框架在单细胞领域是非常重要的工具，接下来可以深入学习一下相关的知识，并且整理分享。


